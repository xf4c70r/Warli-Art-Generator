{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Warli Art Generator"
      ],
      "metadata": {
        "id": "rQ8COoBZayPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import all the necessary libraries and set the device"
      ],
      "metadata": {
        "id": "WIvBVh5WavU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jHYHZnsRfhTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd45d730-c9f0-4871-dced-e27ac2a52d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "id": "LBgRNZr0-kM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e800a2-adf6-47cf-c1b0-3a2aa34b4fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the number of samples in each class. Identify their size and channels"
      ],
      "metadata": {
        "id": "pQqo7ekODwnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Dataset/Warli Art Dataset'\n",
        "\n",
        "for category in os.listdir(data_dir):\n",
        "    if os.path.isdir(os.path.join(data_dir, category)):\n",
        "        category_dir = os.path.join(data_dir, category)\n",
        "        image_files = [f for f in os.listdir(category_dir) if f.endswith(('.jpg'))]\n",
        "        num_images = len(image_files)\n",
        "\n",
        "        print(f\"Category: {category}\")\n",
        "        print(f\"Number of images: {num_images}\")\n",
        "\n",
        "        if num_images > 0:\n",
        "            sample_image_path = os.path.join(category_dir, image_files[0])\n",
        "            sample_image = Image.open(sample_image_path)\n",
        "            image_size = sample_image.size\n",
        "            # num_channels = len(sample_image.mode)\n",
        "            num_channels = len(sample_image.getbands())\n",
        "\n",
        "            print(f\"Image size: {image_size}\")\n",
        "            print(f\"Number of channels: {num_channels}\")\n",
        "\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "C4KrINQ5_X4Z",
        "outputId": "12a54633-e611-4926-806e-1a11fcb247bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/Warli Art Dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c00eddb2c7c4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Dataset/Warli Art Dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcategory_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/Warli Art Dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Writing a data loader\n"
      ],
      "metadata": {
        "id": "0E4gVWlQEAf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example to understand\n",
        "\n",
        "Transform is used to apply multiple diffrent transformation to an image.\n",
        "```\n",
        "transforms.Resize(H, W)\n",
        "```\n",
        "Here, we resize the image to **H x W**. For example: transforms.Resize(64, 64) sets H = 64 and W = 64.\n",
        "\n",
        "Next,\n",
        "```\n",
        "transforms.ToTensor()\n",
        "```\n",
        "This converts the image into tensors.\n",
        "\n",
        "Finally,\n",
        "```\n",
        "transforms.Normalize(mean, std)\n",
        "```\n",
        "Here, the operation would be\n",
        "\n",
        "(Image_Tensor[i][j] - mean[i][j]) / std[i][j]\n",
        "\n",
        "For example if we conider a tensor [0.8, 0.2, 0.5] applying transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5]) would give you a tensor [0.6, -0.6, 0.0]"
      ],
      "metadata": {
        "id": "sBOvX3f1Xkdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/content/drive/MyDrive/Dataset/Warli Art Dataset/Animals/Warli001.jpg')\n",
        "\n",
        "transform1 = transforms.Compose([\n",
        "    # transforms.Resize(64,64),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "transform2 = transforms.Compose([\n",
        "    transforms.Resize(2,2),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "transform3 = transforms.Compose([\n",
        "    transforms.Resize(2,2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "tensor_image1 = transform1(image)\n",
        "tensor_image2 = transform2(image)\n",
        "tensor_image3 = transform3(image)\n",
        "\n",
        "print(\"One: \", tensor_image1)\n",
        "print()\n",
        "print(\"Two: \", tensor_image2)\n",
        "print()\n",
        "print(\"Three: \", tensor_image3)"
      ],
      "metadata": {
        "id": "h09Loz93WXMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f373cb-7cb3-422d-ab52-3d70b2904f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One:  tensor([[[0.8000, 0.7647, 0.7098,  ..., 0.6627, 0.6667, 0.6706],\n",
            "         [0.8235, 0.7922, 0.7451,  ..., 0.6588, 0.6627, 0.6627],\n",
            "         [0.8549, 0.8353, 0.8000,  ..., 0.6588, 0.6549, 0.6549],\n",
            "         ...,\n",
            "         [0.8118, 0.8078, 0.8039,  ..., 0.6784, 0.6784, 0.6784],\n",
            "         [0.8078, 0.8000, 0.7922,  ..., 0.6784, 0.6784, 0.6784],\n",
            "         [0.8039, 0.7961, 0.7882,  ..., 0.6784, 0.6784, 0.6784]],\n",
            "\n",
            "        [[0.7961, 0.7608, 0.7098,  ..., 0.6549, 0.6588, 0.6627],\n",
            "         [0.8196, 0.7882, 0.7412,  ..., 0.6510, 0.6549, 0.6549],\n",
            "         [0.8510, 0.8314, 0.7961,  ..., 0.6510, 0.6471, 0.6471],\n",
            "         ...,\n",
            "         [0.8157, 0.8118, 0.8078,  ..., 0.6588, 0.6588, 0.6588],\n",
            "         [0.8118, 0.8039, 0.7961,  ..., 0.6588, 0.6588, 0.6588],\n",
            "         [0.8078, 0.8000, 0.7922,  ..., 0.6588, 0.6588, 0.6588]],\n",
            "\n",
            "        [[0.7765, 0.7412, 0.6784,  ..., 0.6078, 0.6118, 0.6157],\n",
            "         [0.8039, 0.7686, 0.7216,  ..., 0.6039, 0.6078, 0.6078],\n",
            "         [0.8431, 0.8157, 0.7804,  ..., 0.6039, 0.6000, 0.6000],\n",
            "         ...,\n",
            "         [0.8235, 0.8196, 0.8157,  ..., 0.6353, 0.6353, 0.6353],\n",
            "         [0.8196, 0.8118, 0.8039,  ..., 0.6353, 0.6353, 0.6353],\n",
            "         [0.8157, 0.8078, 0.8000,  ..., 0.6353, 0.6353, 0.6353]]])\n",
            "\n",
            "Two:  tensor([[[0.6588, 0.5216, 0.5725],\n",
            "         [0.6745, 0.5765, 0.6118]],\n",
            "\n",
            "        [[0.6471, 0.4275, 0.5176],\n",
            "         [0.6667, 0.5216, 0.5765]],\n",
            "\n",
            "        [[0.6392, 0.4118, 0.4863],\n",
            "         [0.6627, 0.5059, 0.5529]]])\n",
            "\n",
            "Three:  tensor([[[ 0.3176,  0.0431,  0.1451],\n",
            "         [ 0.3490,  0.1529,  0.2235]],\n",
            "\n",
            "        [[ 0.2941, -0.1451,  0.0353],\n",
            "         [ 0.3333,  0.0431,  0.1529]],\n",
            "\n",
            "        [[ 0.2784, -0.1765, -0.0275],\n",
            "         [ 0.3255,  0.0118,  0.1059]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataloader\n",
        "```\n",
        "ImageFolder(data_pth, transfrom=transform)\n",
        "```\n",
        "Image folder is used to load data (image in this case) form **dir** where the **dir** is the category of the image\n",
        "\n",
        "It encodes the categories as an integer\n",
        "\n",
        "like category1 = 0, category2 = 1, etc..\n",
        "\n",
        "It also maintains a mapping of class to integers\n",
        "\n"
      ],
      "metadata": {
        "id": "MRRcHwAWWeSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "# Path to data\n",
        "data_pth = '/content/drive/MyDrive/Dataset/Warli Art Dataset'\n",
        "\n",
        "# Dataloader\n",
        "\n",
        "# Dataloader\n",
        "dataset = ImageFolder(data_pth, transform=transform)\n",
        "total_samples = len(dataset)\n",
        "train_samples = int(0.5 * total_samples)  # 60% of the total samples\n",
        "val_samples = total_samples - train_samples\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, val_dataset = random_split(dataset, [train_samples, val_samples])\n",
        "\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "gAQ-xNMgEAGy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ecb482ed-2ebc-46e9-9d9e-cde6c2012a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/Warli Art Dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e80b6bddd12b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrain_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 60% of the total samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Dataset/Warli Art Dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Images to verify\n",
        "1.  Undo the normalization\n",
        "```\n",
        "img = img / 2 + 0.5\n",
        "```\n",
        "\n",
        "2. Convert the PyTorch tensor to numpy array\n",
        "```\n",
        "npimg = img.numpy()\n",
        "```\n",
        "\n",
        "3. Transpose the image in (H x W x C) format\n",
        "```\n",
        "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "```\n",
        "\n",
        "4. This arranges the images in batches and shows one batchof images\n",
        "```\n",
        "torchvision.utils.make_grid(images)\n",
        "```"
      ],
      "metadata": {
        "id": "nQwbyIkWb3jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Images to verify\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "metadata": {
        "id": "C3OtZveNM9l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAN"
      ],
      "metadata": {
        "id": "C6rKERREaTEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generator"
      ],
      "metadata": {
        "id": "YfThVjesaYaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Input is the latent vector Z.\n",
        "            nn.ConvTranspose2d(100, 1024, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # Additional Conv layer\n",
        "            nn.ConvTranspose2d(1024, 1024, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # State size. (1024x4x4)\n",
        "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # Additional Conv layer\n",
        "            nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            # State size. (512x8x8)\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "            # State size. (256x16x16)\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "            # State size. (128x32x32)\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.4),\n",
        "            # State size. (64x64x64)\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()  # Tanh activation to output images with pixel values in range [-1, 1]\n",
        "            # Final state size. (3x128x128)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 100, 1, 1)  # Reshape input noise vector into a batch of inputs for ConvTranspose2d\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "o7L7ZSnTdbZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discriminator"
      ],
      "metadata": {
        "id": "O9MHv9FoacHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            # Input size. (3x128x128)\n",
        "            nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (64x64x64)\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (128x32x32)\n",
        "            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (256x16x16)\n",
        "            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (512x8x8)\n",
        "            nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (1024x4x4)\n",
        "            nn.Conv2d(1024, 1, 4, stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()  # Output a single scalar per image indicating real or fake\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output.view(-1, 1)  # Flatten to [batch_size, 1]"
      ],
      "metadata": {
        "id": "s-ggfA5RdcSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Models"
      ],
      "metadata": {
        "id": "b4jdSGwrjluh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator().to(device)\n",
        "dis = Discriminator().to(device)"
      ],
      "metadata": {
        "id": "OTaSMABZjmoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "uhXXV3JKaffk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_epochs, dataloader, optimizer_dis, optimizer_gen, dis, gen):\n",
        "  for epoch in range(n_epochs):\n",
        "        # Initialize losses\n",
        "        d_loss = 0.0\n",
        "        g_loss = 0.0\n",
        "        num_batch = 0\n",
        "\n",
        "        for i, (real_image, _) in enumerate(dataloader):\n",
        "          real_image = real_image.to(device)\n",
        "          batch_size = real_image.size(0)\n",
        "\n",
        "          # Train Discriminator\n",
        "          optimizer_dis.zero_grad()\n",
        "\n",
        "          # Real Data\n",
        "          real_pred = dis(real_image)\n",
        "          real_label = torch.ones_like(real_pred, device=device, dtype=torch.float32)\n",
        "          real_loss =  F.binary_cross_entropy(real_pred, real_label, reduction='mean')\n",
        "\n",
        "          # Fake Data\n",
        "          noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "          gen_out = gen(noise)\n",
        "          fake_pred = dis(gen_out)\n",
        "          fake_label = torch.zeros_like(fake_pred, device=device, dtype=torch.float32)\n",
        "          fake_loss =  F.binary_cross_entropy(fake_pred, fake_label, reduction='mean')\n",
        "\n",
        "          dis_loss = (real_loss+fake_loss)/2\n",
        "          dis_loss.backward()\n",
        "          optimizer_dis.step()\n",
        "\n",
        "          # Train Generator\n",
        "          optimizer_gen.zero_grad()\n",
        "\n",
        "          noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
        "          gen_out = gen(noise)\n",
        "          dis_out = dis(gen_out)\n",
        "          # label = real_label = torch.ones_like(dis_out, device=device, dtype=torch.float32)\n",
        "          gen_loss =  F.binary_cross_entropy(dis_out, real_label, reduction='mean')\n",
        "\n",
        "          gen_loss.backward()\n",
        "          optimizer_gen.step()\n",
        "\n",
        "          # Accumulate losses\n",
        "          d_loss += dis_loss.item()\n",
        "          g_loss += gen_loss.item()\n",
        "          num_batch += 1\n",
        "\n",
        "        # Print losses\n",
        "        # if i % 1 == 0:\n",
        "        avg_d_loss = d_loss/num_batch\n",
        "        avg_g_loss = g_loss/num_batch\n",
        "        print(f\"Epoch [{epoch + 1}] Loss D: {avg_d_loss:.4f} Loss G: {avg_g_loss:.4f}\")"
      ],
      "metadata": {
        "id": "4cGaspGvabmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10  # Number of epochs for training\n",
        "# lr = 0.0001  # Learning rate\n",
        "optimizer_dis = optim.Adam(dis.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "Uvkop14Np8IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(n_epochs, dataloader, optimizer_dis, optimizer_gen, dis, gen)"
      ],
      "metadata": {
        "id": "_IfL0WOUp0hW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ab43425a-1485-42c6-a79c-d55f1b9346b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f491532d7a8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try out the Generator"
      ],
      "metadata": {
        "id": "rlCsV9Huai4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "gen.eval()\n",
        "\n",
        "num_images = 16  # Number of images to generate\n",
        "latent_dim = 100  # Dimension of the latent vector\n",
        "noise = torch.randn(num_images, latent_dim, 1, 1)  # Generate random noise\n",
        "\n",
        "# Generate images from the noise\n",
        "# Ensure that noise is on the same device as the model\n",
        "noise = noise.to(next(gen.parameters()).device)  # Move noise to the device of the model\n",
        "fake_images = gen(noise)\n",
        "\n",
        "# Convert images to a suitable format for displaying\n",
        "fake_images = (fake_images + 1) / 2  # Adjust from [-1, 1] to [0, 1]\n",
        "grid = make_grid(fake_images, nrow=4)  # Create a grid of images\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().detach().numpy())  # Convert to numpy and plot\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jeKJ5JqbdZbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f51e252-772b-4b4a-f6e4-0f0bb87fd991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv_transpose2d(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAMWCAYAAABsvhCnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUrklEQVR4nO3bP4ucdduA4Wszu2E3jUZBUBNNYS0KaUKsrWzFwo/gp7GwtxAEP4BgI4ikEhE/QMgfMTw2axqzYTez81bPvtrteOKMefY4mv0tzDVcxb0w59737KxWq9UAAAAEl7a9AAAA8PwTFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAINs97wt3dnb+yT0AAIB/odVqda7XuWMBAABk575j8V87Oztz5coVdzC4MJbL5RwdHc3MzGKxmIODgy1vBJtzfHw8x8fHMzNz+fLluXz58pY3gs05Ojqa5XI5MzMHBwezWCy2vBFsxmq1midPnpz7TsV/rR0WV65cmU8++WR2d9cehefSw4cP54svvpiZmRs3bsxHH3205Y1gc+7cuTPffffdzMzcvn17bt26teWNYHO+/PLLuXfv3szMfPjhh3Pt2rUtbwSbcXJyMp999tk8efJkrbm/dcdid3d39vb21h2F59KfI3pnZ8e1z4Xy5//QLhYL1z8XyqVL///EuM8+XDR/5+kk37EAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABku+sOLJfL+eWXX2Z3d+1ReC799ttvZ+ejo6N5+PDhFreBzXr8+PFfzq5/LpKjo6Oz82+//Tar1WqL28DmPHv2bJbL5dpzO6tz/pXs7Oys/eYAAMDz7bxR7VEoAAAgW/t5psViMTdu3HAHgwvj6Ohofv3115mZuXLlyrz22mtb3gg25/DwcA4PD2dm5uWXX56rV69ueSPYnEePHs2TJ09mZubatWuzv7+/5Y1gM05PT+fBgwdrPw61dlgcHBzMRx99NHt7e+uOwnPpwYMH8/nnn8/MzGuvvTYff/zxdheCDfr+++/n22+/nZmZd999d27fvr3ljWBzvvjii7l79+7MzLz//vtz/fr1LW8Em3FycjKffvrp/PHHH2vNeRQKAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACDbXXfg+Ph47ty5M4vF4p/YB/51Hj9+fHY+PDyc77//fnvLwIbdv3//7Hzv3r05PT3d3jKwYb///vvZ+eeff/7L3wP8L1sul3N8fLz23M5qtVqd64U7O2u/OQAA8Hw7Zy54FAoAAOjWfhTq8uXLc/v27bl0SZNwMTx+/Hh+/PHHmZl5+eWX55133tnuQrBB9+/fn7t3787MzFtvvTVvvvnmljeCzfnpp5/m8PBwZmZu3rw5L7zwwpY3gs1YLpdz586dOTk5WWvub4XFrVu3Zm9vb91ReC49ePDgLCyuXr0677333pY3gs1ZrVZnYXHjxo25ffv2ljeCzbl///5ZWLz99ttz/fr1LW8Em3FycjI//PDD2mHhtgMAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQLazWq1W53rhzs7MzCwWi3njjTfm0iVNwsVwdHQ0jx49mpmZK1euzKuvvrrljWBzfv/99zk8PJyZmZdeemmuXr265Y1gcx49ejRHR0czM/P666/P/v7+ljeCzTg9PZ2HDx/OcrmcmZlz5sL6YQEAAFwc5w0Ltx0AAIBsd92Bg4OD+fDDD2d3d+1ReC795z//ma+//npmZq5duzbvv//+ljeCzfn555/nxx9/nJmZmzdvzttvv73ljWBzvvnmm/n1119nZuaDDz6YV155ZcsbwWY8e/Zsvvrqq3n69Olac2vXwWKxmGvXrs3e3t66o/BcOj09PTvv7+/P9evXt7gNbNb9+/fPzi+++KLrnwvlz9+peOWVV1z/XBgnJyezWCzWnvMoFAAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAIBMWAABAJiwAAIBMWAAAAJmwAAAAMmEBAABkwgIAAMiEBQAAkAkLAAAgExYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQCYsAACATFgAAACZsAAAADJhAQAAZMICAADIhAUAAJAJCwAAINtdd2C1Ws3Jyck/sQv8Kz179uzsfHp66vrnQlkul385u/65SE5PT8/Oz549c/1zYRwfH89qtVp7bmd1zqmdnZ2znwcHB2e/w/+65XI5T58+nZmZxWIx+/v7W94INuf4+Pjsw9Te3t5cvnx5yxvB5jx9+vQsrvf392exWGx5I9iM1Wo1T548+cvv57F2WAAAABfHecPCdywAAIDs3N+x+DvPWQEAABeDOxYAAEAmLAAAgExYAAAAmbAAAAAyYQEAAGTCAgAAyIQFAACQCQsAACATFgAAQPZ/DZqjczo/vQAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}