{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8686085,"sourceType":"datasetVersion","datasetId":5207794}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, random_split\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device: \", device)\n\ndirectory = '/kaggle/input/warli-art/Warli Art Object Image Dataset/Warli Art Dataset/Warli Art Dataset'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = directory\n\nfor category in os.listdir(data_dir):\n    if os.path.isdir(os.path.join(data_dir, category)):\n        category_dir = os.path.join(data_dir, category)\n        image_files = [f for f in os.listdir(category_dir) if f.endswith(('.jpg'))]\n        num_images = len(image_files)\n\n        print(f\"Category: {category}\")\n        print(f\"Number of images: {num_images}\")\n\n        if num_images > 0:\n            sample_image_path = os.path.join(category_dir, image_files[0])\n            sample_image = Image.open(sample_image_path)\n            image_size = sample_image.size\n            # num_channels = len(sample_image.mode)\n            num_channels = len(sample_image.getbands())\n\n            print(f\"Image size: {image_size}\")\n            print(f\"Number of channels: {num_channels}\")\n\n        print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transforms\ntransform = transforms.Compose([\n    torchvision.transforms.CenterCrop((600, 600)),\n    transforms.Resize((128,128)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n])\n\n# Path to data\ndata_pth = directory\n\n# Dataloader\ndataset = ImageFolder(data_pth, transform=transform)\n# total_samples = len(dataset)\n# train_samples = int(0.5 * total_samples)  # 50% of the total samples\n# val_samples = total_samples - train_samples\n\n# Split the dataset into training and validation sets\n# train_dataset, val_dataset = random_split(dataset, [train_samples, val_samples])\n\nbatch_size = 64\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimages,_ = next(iter(dataloader))\n\nplt.figure(figsize=(8, 8))\nplt.axis(\"off\")\nplt.title(\"Images\")\nplt.imshow(np.transpose(torchvision.utils.make_grid(images[:64], \n                                         padding=2, normalize=True),\n                        (1, 2, 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Residual Block\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n        nn.Conv2d(in_feat, out_feat, 3, stride=1, padding=1, bias=False),\n        nn.BatchNorm2d(out_feat),\n        nn.ReLU(inplace=True), \n        nn.Conv2d(out_feat, out_feat, 3, stride=1, padding=1, bias=False),\n        nn.BatchNorm2d(out_feat),\n        nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return x + self.block(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilon=0.00005\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            # Input is the latent vector Z.\n            nn.ConvTranspose2d(100, 1024, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(1024, eps=epsilon),\n            nn.ReLU(True),\n            nn.Dropout(0.4),\n\n#             # Additional Conv layer\n#             nn.ConvTranspose2d(1024, 1024, 3, 1, 1, bias=False),\n#             nn.BatchNorm2d(1024, eps=epsilon),\n#             nn.ReLU(True),\n#             nn.Dropout(0.4),\n            \n            ResBlock(1024,1024),\n\n            # State size. (1024x4x4)\n            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512, eps=epsilon),\n            nn.ReLU(True),\n            nn.Dropout(0.4),\n\n#             # Additional Conv layer\n#             nn.ConvTranspose2d(512, 512, 3, 1, 1, bias=False),\n#             nn.BatchNorm2d(512, eps=epsilon),\n#             nn.ReLU(True),\n#             nn.Dropout(0.4),\n            \n            ResBlock(512,512),\n\n            # State size. (512x8x8)\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256, eps=epsilon),\n            nn.ReLU(True),\n            nn.Dropout(0.4),\n\n            # State size. (256x16x16)\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128, eps=epsilon),\n            nn.ReLU(True),\n            nn.Dropout(0.4),\n            \n            ResBlock(128,128),\n\n            # State size. (128x32x32)\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64, eps=epsilon),\n            nn.ReLU(True),\n            nn.Dropout(0.4),\n\n            # State size. (64x64x64)\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()  \n            # Final state size. (3x128x128)\n        )\n\n    def forward(self, x):\n        x = x.view(-1, 100, 1, 1)  # Reshape input noise vector into a batch of inputs for ConvTranspose2d\n        output = self.model(x)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n\n            # Input size. (3x128x128)\n            nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n\n            # State size. (64x64x64)\n            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n            \n#             ResBlock(128,128),\n\n            # State size. (128x32x32)\n            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n            \n#             ResBlock(256,256),\n\n            # State size. (256x16x16)\n            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n            \n#             ResBlock(512,512),\n\n            # State size. (512x8x8)\n            nn.Conv2d(512, 1024, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n            \n            # State size. (1024x4x4)\n            nn.Conv2d(1024, 1, 4, stride=1, padding=0, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.4),\n            nn.Sigmoid()  # Output a single scalar per image indicating real or fake\n        )\n\n    def forward(self, x):\n        output = self.model(x)\n        return output.view(-1, 1)  # Flatten to [batch_size, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndef train(n_epochs, dataloader, optimizer_dis, optimizer_gen, dis, gen):\n  print(f\"Starting Training for {n_epochs} Epochs\")\n  for epoch in range(n_epochs):\n        start = time.time()\n        # Initialize losses\n        d_loss = 0.0\n        g_loss = 0.0\n        num_batch = 0\n\n        for i, (real_image, _) in enumerate(dataloader):\n          real_image = real_image.to(device)\n          batch_size = real_image.size(0)\n          # Add noise to the inputs\n          noise_real = torch.randn_like(real_image) * 0.1\n          noise_fake = torch.randn_like(real_image) * 0.1\n\n          # Train Discriminator\n          optimizer_dis.zero_grad()\n          real_image_noisy = real_image + noise_real\n          # Real Data\n          real_pred = dis(real_image_noisy)\n#           real_label = torch.ones_like(real_pred, device=device, dtype=torch.float32)\n          real_label = torch.full_like(real_pred, 0.9, device=device, dtype=torch.float32)  # Smoothed label\n          real_loss =  F.binary_cross_entropy(real_pred, real_label, reduction='mean')\n\n          # Fake Data\n          noise = torch.randn(batch_size, 100, 1, 1, device=device)\n          gen_out = gen(noise)\n          fake_image_noisy = gen_out + noise_fake\n          fake_pred = dis(fake_image_noisy)\n#           fake_label = torch.zeros_like(fake_pred, device=device, dtype=torch.float32)\n          fake_label = torch.full_like(fake_pred, 0.1, device=device, dtype=torch.float32)  # Smoothed label\n          fake_loss =  F.binary_cross_entropy(fake_pred, fake_label, reduction='mean')\n\n          dis_loss = (real_loss+fake_loss)/2\n          dis_loss.backward()\n          optimizer_dis.step()\n\n          # Train Generator\n          optimizer_gen.zero_grad()\n\n          noise = torch.randn(batch_size, 100, 1, 1, device=device)\n          gen_out = gen(noise)\n          dis_out = dis(gen_out)\n          # label = real_label = torch.ones_like(dis_out, device=device, dtype=torch.float32)\n          gen_loss =  F.binary_cross_entropy(dis_out, real_label, reduction='mean')\n\n          gen_loss.backward()\n          optimizer_gen.step()\n\n          # Accumulate losses\n          d_loss += dis_loss.item()\n          g_loss += gen_loss.item()\n          num_batch += 1\n\n\n        # Print losses\n        end = time.time()\n#         if i % 10 == 0:\n        avg_d_loss = d_loss/num_batch\n        avg_g_loss = g_loss/num_batch\n        print(f\"Epoch [{epoch + 1}] Loss D: {avg_d_loss:.4f} Loss G: {avg_g_loss:.4f} Time: {end-start:.2f} sec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen = Generator().to(device)\ndis = Discriminator().to(device)\n\ngen = nn.DataParallel(gen)\ndis = nn.DataParallel(dis)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 80# Number of epochs for training\n# optimizer_dis = optim.Adam(dis.parameters(), lr=0.00004, betas=(0.5, 0.999))\noptimizer_dis = optim.RMSprop(dis.parameters(), lr=0.00002)\n# optimizer_gen = optim.Adam(gen.parameters(), lr=0.0004, betas=(0.5, 0.999))\noptimizer_gen = optim.Adagrad(gen.parameters(), lr=0.02)\n# optimizer_gen = optim.RMSprop(gen.parameters(), lr=0.02)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(n_epochs, dataloader, optimizer_dis, optimizer_gen, dis, gen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\n# Set the model to evaluation mode\ngen.eval()\n\nnum_images = 16  # Number of images to generate\nlatent_dim = 100  # Dimension of the latent vector\nnoise = torch.randn(num_images, latent_dim, 1, 1)  # Generate random noise\n\n# Generate images from the noise\n# Ensure that noise is on the same device as the model\nnoise = noise.to(next(gen.parameters()).device)  # Move noise to the device of the model\nfake_images = gen(noise)\n\n# Convert images to a suitable format for displaying\nfake_images = (fake_images + 1) / 2  # Adjust from [-1, 1] to [0, 1]\ngrid = make_grid(fake_images, nrow=4)  # Create a grid of images\n\n# Plot the images\nplt.figure(figsize=(10, 10))\nplt.imshow(grid.permute(1, 2, 0).cpu().detach().numpy())  # Convert to numpy and plot\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}